---
title: "Machine Learning for Business Analytics"
author: "Chapter 13: Generating, Comparing, and Combining Multiple Models"
output:
  pdf_document:
    toc: no
    highlight: tango
#  html_document:
#    toc: yes
#    toc_depth: 4
#    toc_float: yes
---
<style>
h1.title { font-size: 28px; }
h1 { font-size: 22px; }
h2 { font-size: 18px; }
h3 { font-size: 14px; }
h4 { font-size: 12px; }
</style>
```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE,     # do not show R messages
                      row.print=25)
```


```{r}
if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force=TRUE)
}
```

# Ensembles
## Bagging and Boosting in R
### Combining Propensities
```{r}
library(tidyverse)
library(adabag)
library(rpart)
library(caret)

set.seed(1)
# load and preprocess the data
bank.df <- mlba::UniversalBank %>%
    select(-c(ID, ZIP.Code)) %>%
    mutate(
      Personal.Loan = factor(Personal.Loan, levels=c(0, 1), labels=c("No", "Yes"))
    )

# partition the data
train.index <- sample(c(1:dim(bank.df)[1]), dim(bank.df)[1]*0.6)
train.df <- bank.df[train.index, ]
holdout.df <- bank.df[-train.index, ]

# single tree (rpart)
tr <- rpart(Personal.Loan ~ ., data=train.df)

# bagging and boosting using adabag
bag <- bagging(Personal.Loan ~ ., data=train.df)
boost <- boosting(Personal.Loan ~ ., data=train.df)

# bagging and boosting using randomForest and xgboost with parameter tuning
bag.rf <- train(Personal.Loan ~ ., data=train.df, method="rf")
boost.xgb <- train(Personal.Loan ~ ., data=train.df, method="xgbTree", verbosity=0)
```

```{r}
library(ROCR)
rocCurveData <- function(prob, data) {
  predob <- prediction(prob, data$Personal.Loan)
  perf <- performance(predob, "tpr", "fpr")
  return (data.frame(tpr=perf@x.values[[1]], fpr=perf@y.values[[1]]))
}
performance.df <- rbind(
  cbind(rocCurveData(predict(tr, holdout.df, type="prob")[,"Yes"], holdout.df), model="Single tree"),
  cbind(rocCurveData(predict(bag, holdout.df)$prob[, 2], holdout.df), model="Bagging"),
  cbind(rocCurveData(predict(boost, holdout.df)$prob[, 2], holdout.df), model="Boosting")
)
colors <- c("Single tree"="grey", "Bagging"="blue", "Boosting"="tomato")
g1 <- ggplot(performance.df, aes(x=tpr, y=fpr, color=model)) +
  geom_line() +
  scale_color_manual(values=colors) +
  geom_segment(aes(x=0, y=0, xend=1, yend=1), color="grey", linetype="dashed") +
  labs(x="1 - Specificity", y="Sensitivity", color="Model")
g1

performance.df <- rbind(
  cbind(rocCurveData(predict(tr, holdout.df, type="prob")[,"Yes"], holdout.df),
        model="Single tree"),
  cbind(rocCurveData(predict(bag.rf, holdout.df, type="prob")[,"Yes"], holdout.df),
        model="Random Forest"),
  cbind(rocCurveData(predict(boost.xgb, holdout.df, type="prob")[,"Yes"], holdout.df),
        model="xgboost")
)
colors <- c("Single tree"="grey", "Random Forest"="blue", "xgboost"="tomato")
g2 <- ggplot(performance.df, aes(x=tpr, y=fpr, color=model)) +
  geom_line() +
  scale_color_manual(values=colors) +
  geom_segment(aes(x=0, y=0, xend=1, yend=1), color="grey", linetype="dashed") +
  labs(x="1 - Specificity", y="Sensitivity", color="Model")
g2
library(gridExtra)
g <- arrangeGrob(g1 + theme_bw(), g2 + theme_bw(), ncol=2, widths=c(0.49, 0.51))
ggsave(file=file.path("..", "figures", "chapter_13", "bagging-boosting.pdf"),
       g, width=8, height=3, units="in")
```

# Automated Machine Learning (AutoML)
## AutoML: Explore and Clean Data
```{r}
library(tidyverse)

# load and preprocess the data
bank.df <- mlba::UniversalBank %>%
  # Drop ID and zip code columns.
  select(-c(ID, ZIP.Code)) %>%
  # convert Personal.Loan to a factor with labels Yes and No
  mutate(Personal.Loan = factor(Personal.Loan, levels=c(0, 1), labels=c("No", "Yes")))

# partition the data
set.seed(1)
idx <- caret::createDataPartition(bank.df$Personal.Loan, p=0.6, list=FALSE)
train.df <- bank.df[idx, ]
holdout.df <- bank.df[-idx, ]
```

```{r}
library(h2o)

# Start the H2O cluster (locally)
h2o.init()

train.h2o <- as.h2o(train.df)
holdout.h2o <- as.h2o(holdout.df)
```

```{r}

```

## AutoML: Choose Features and Machine Learning Methods
```{r}
# identify outcome and predictors
y <- "Personal.Loan"
x <- setdiff(names(train.df), y)

# run AutoML for 20 base models
aml <- h2o.automl(x=x, y=y, training_frame=train.h2o,
                  max_models=20, exclude_algos=c("DeepLearning"),
                  seed=1)
aml.balanced <- h2o.automl(x=x, y=y, training_frame=train.h2o,
                  max_models=20, exclude_algos=c("DeepLearning"),
                  balance_classes=TRUE,
                  seed=1)

aml
```

```{r}
aml.balanced
```

## AutoML: Evaluate Model Performance
```{r}
h2o.confusionMatrix(aml@leader, holdout.h2o)
h2o.confusionMatrix(aml.balanced@leader, holdout.h2o)
```

# Explaining Model Predictions
## Explaining Model Predictions: LIME
```{r}
cases <- c('3055', '3358', # predicted Yes
           '2', '1178')    # predicted No
explainer <- lime::lime(train.df, aml@leader, bin_continuous=TRUE, quantile_bins=FALSE)
explanations <- lime::explain(holdout.df[cases,], explainer, n_labels=1, n_features=8)

lime::plot_features(explanations, ncol=2)
```

```{r}
pdf(file=file.path("..", "figures", "chapter_13", "lime-analysis.pdf"),
    width=7, height=6)
    lime::plot_features(explanations, ncol=2)
dev.off()
```

